{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RAptMWPCryKy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    recall_score,\n","    precision_score,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay,\n","    f1_score\n",")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from google.colab import drive"]},{"cell_type":"markdown","source":["##a"],"metadata":{"id":"S9QTDcdLMxM_"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"UsIDCbkyr1vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = '/content/drive/MyDrive/Ciencia_de_Datos/Tarea3/loan_data.csv'\n","df = pd.read_csv(url)"],"metadata":{"id":"H9YjkWr-vFXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##b"],"metadata":{"id":"_7Pz9ZytMzGW"}},{"cell_type":"code","source":["ndf = pd.get_dummies(\n","    df, prefix='purpose', prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None\n",")"],"metadata":{"id":"phlmsD6JM0E8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ndf.info()"],"metadata":{"id":"zXo8xc5UKz6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = ndf.loc[:, ndf.columns != 'not.fully.paid']\n","y = ndf['not.fully.paid']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.33, random_state=125\n",")"],"metadata":{"id":"gWU4ZVcwM3z8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##c"],"metadata":{"id":"pDdvenFkM7Wl"}},{"cell_type":"code","source":["copia_train =  X_train.copy()\n","copia_train['clase'] = y_train.copy()\n","\n","train0 = copia_train[copia_train['clase'] == 0]\n","train1 = copia_train[copia_train['clase'] == 1]\n","train0 = train0.drop('clase', axis=1)\n","train1= train1.drop('clase', axis=1)\n","\n","mean0 = train0.mean()\n","mean1 = train1.mean()\n","\n","covariance_matrix0 = train0.cov(ddof=0)\n","covariance_matrix1 = train1.cov(ddof=0)\n","\n","fraction_class0 = np.mean(y_train == 0)\n","fraction_class1 = np.mean(y_train == 1)\n","\n","print('Clase 1:')\n","print(f'La media muestral de todas las variables es:\\n\\n{mean0}')\n","print(f'Proporción muestral: {fraction_class0}')\n","\n","print('--------------------------------------------')\n","\n","print('Clase 2:')\n","print(f'La media muestral de todas las variables es:\\n\\n{mean1}')\n","print(f'Proporción muestral: {fraction_class1}')"],"metadata":{"id":"3h3fr6UFM8VZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ACLARACIÓN: No imprimí las matrices de covarianza en la celda anterior porque estas son 18x18."],"metadata":{"id":"GUFs30m2Nf02"}},{"cell_type":"code","source":["model = GaussianNB()\n","\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","accuray = accuracy_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred, average=\"weighted\")\n","recall = recall_score(y_test, y_pred, pos_label=1)\n","precision = precision_score(y_test, y_pred, pos_label=1)\n","\n","print('Accuracy:', accuray)\n","print('F1 Score:', f1)\n","print('Recall :', recall)\n","print('Precision :', precision)"],"metadata":{"id":"KqVM4fj-OAjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = [\"Fully Paid\", \"Not fully Paid\"]\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot()"],"metadata":{"id":"yUg0IhvUOCm5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##d"],"metadata":{"id":"q4AC05cWUPik"}},{"cell_type":"code","source":["#Declaro la lista de columnas asi en ves de usar ndf.columns para ya tener englobados todos los\n","# propositos (purpose) en una sola columna y asi armar mi tabla mas facil despues\n","columnas = [\n","    \"none\", \"credit.policy\", \"int.rate\", \"installment\", \"log.annual.inc\", \"dti\",\n","    \"fico\", \"days.with.cr.line\", \"revol.bal\", \"revol.util\", \"inq.last.6mths\",\n","    \"delinq.2yrs\", \"pub.rec\", \"purpose\"\n","]\n","\n","f1_scores = [f1]\n","accuracys = [accuray]\n","recalls = [recall]\n","precisions = [precision]\n","\n","purposes = [\n","    \"purpose_credit_card\",\n","    \"purpose_debt_consolidation\", \"purpose_educational\", \"purpose_home_improvement\",\n","    \"purpose_major_purchase\", \"purpose_small_business\"\n","]\n","\n","\n","for c in columnas:\n","    if c != \"none\" and c != 'purpose':\n","\n","        X_trainModified = X_train.loc[:, X_train.columns != c]\n","        X_testModified = X_test.loc[:, X_test.columns != c]\n","\n","        model = GaussianNB()\n","\n","        model.fit(X_trainModified, y_train)\n","\n","        y_pred = model.predict(X_testModified)\n","\n","        f1_scores.append(f1_score(y_test, y_pred, average=\"weighted\"))\n","        accuracys.append(accuracy_score(y_test, y_pred))\n","        precisions.append(precision_score(y_test, y_pred, pos_label=1))\n","        recalls.append(recall_score(y_test, y_pred, pos_label=1))\n","\n","    elif c == 'purpose':\n","\n","        X_trainModified = X_train.loc[:, ~X_train.columns.isin(purposes)]\n","        X_test_Modified = X_test.loc[:, ~X_train.columns.isin(purposes)]\n","\n","        model = GaussianNB()\n","        model.fit(X_trainModified, y_train)\n","\n","        y_pred = model.predict(X_test_Modified)\n","\n","        f1_scores.append(f1_score(y_test, y_pred, average=\"weighted\"))\n","        accuracys.append(accuracy_score(y_test, y_pred))\n","        precisions.append(precision_score(y_test, y_pred, pos_label=1))\n","        recalls.append(recall_score(y_test, y_pred, pos_label=1))\n","\n","\n","data = {\n","    'col.eliminated' : columnas,\n","    'accuracy' : accuracys,\n","    'recall' : recalls,\n","    'precision' : precisions,\n","    'f1-score' : f1_scores\n","}\n","\n","tabla = pd.DataFrame(data)"],"metadata":{"id":"O3pZr8N1OHFj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tabla)"],"metadata":{"id":"Oz2eZRZpOJgU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##e"],"metadata":{"id":"36m9oKu1ORS0"}},{"cell_type":"code","source":["def ajustar_data(Columnas):\n","    if 'purpose' not in Columnas:\n","        X_modified = X_train.loc[:, ~X_train.columns.isin(Columnas)]\n","        X_testM = X_test.loc[:, ~X_train.columns.isin(Columnas)]\n","\n","        model = GaussianNB()\n","\n","        model.fit(X_modified, y_train)\n","\n","        y_pred = model.predict(X_testM)\n","    else:\n","        X_modified = X_train.loc[:, ~X_train.columns.isin(Columnas)]\n","        X_testM = X_test.loc[:, ~X_train.columns.isin(Columnas)]\n","        X_modified = X_modified.loc[:, ~X_modified.columns.isin(purposes)]\n","        X_testM = X_testM.loc[:, ~X_testM.columns.isin(purposes)]\n","\n","        model = GaussianNB()\n","\n","        model.fit(X_modified, y_train)\n","\n","        y_pred = model.predict(X_testM)\n","\n","    return recall_score(y_test, y_pred), f1_score(y_test, y_pred, average=\"weighted\")"],"metadata":{"id":"mzivgIFwON5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C1 = ['revol.bal', 'revol.util']\n","C2 = ['revol.bal', 'revol.util', 'inq.last.6mths']\n","C3 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'purpose']\n","C4 = ['revol.bal', 'inq.last.6mths', 'purpose']\n","C5 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'int.rate']\n","C6 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'log.annual.inc']\n","C7 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'int.rate', 'delinq.2yrs']\n","C8 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'int.rate', 'delinq.2yrs', 'dti']\n","C9 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'int.rate', 'delinq.2yrs', 'dti', 'days.with.cr.line']\n","C10 = ['revol.bal', 'revol.util', 'inq.last.6mths', 'int.rate', 'delinq.2yrs', 'dti', 'days.with.cr.line', 'fico']\n","C11 = ['revol.bal', 'revol.util','delinq.2yrs', 'days.with.cr.line', 'log.annual.inc',\n","       \"purpose_credit_card\", \"purpose_debt_consolidation\", \"purpose_educational\",\n","       \"purpose_home_improvement\", \"purpose_major_purchase\", \"purpose_small_business\"]\n","C12 = ['revol.bal', 'revol.util','delinq.2yrs', 'days.with.cr.line', 'log.annual.inc', 'pub.rec',\n","       \"purpose_credit_card\", \"purpose_debt_consolidation\", \"purpose_educational\",\n","       \"purpose_home_improvement\", \"purpose_major_purchase\", \"purpose_small_business\"]\n","C13 = ['revol.bal', 'revol.util','delinq.2yrs', 'days.with.cr.line', 'log.annual.inc', 'pub.rec', 'int.rate',\n","       \"purpose_credit_card\", \"purpose_debt_consolidation\", \"purpose_educational\",\n","       \"purpose_home_improvement\", \"purpose_major_purchase\", \"purpose_small_business\"]\n","C14 = ['revol.bal', 'revol.util','delinq.2yrs', 'days.with.cr.line', 'log.annual.inc', 'pub.rec', 'installment',\n","       \"purpose_credit_card\", \"purpose_debt_consolidation\", \"purpose_educational\",\n","       \"purpose_home_improvement\", \"purpose_major_purchase\", \"purpose_small_business\"]\n","C15 = ['revol.bal', 'revol.util','delinq.2yrs', 'days.with.cr.line', 'log.annual.inc', 'pub.rec', 'installment', 'inq.last.6mths',\n","       \"purpose_credit_card\", \"purpose_debt_consolidation\", \"purpose_educational\",\n","       \"purpose_home_improvement\", \"purpose_major_purchase\", \"purpose_small_business\"]\n","C16 = ['revol.bal', 'revol.util','delinq.2yrs', 'days.with.cr.line', 'log.annual.inc', 'pub.rec', 'installment', 'inq.last.6mths','inq.last.6mths',\n","       \"purpose_credit_card\", \"purpose_debt_consolidation\", \"purpose_educational\",\n","       \"purpose_home_improvement\", \"purpose_major_purchase\", \"purpose_small_business\"]\n","\n","Cs = [C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15, C16]\n","for i in Cs:\n","    print(ajustar_data(i), '# Columnas eliminadas :', len(i))"],"metadata":{"id":"y78X-9E1OVgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Por las pruebas hechas anteriormente, noto que puedo eliminar los atributos de la lista C9 y aun asi tener una buena\n","#clasificación, incluso mejorando el recall_score.\n","\n","print('Los atributos eliminados son : \\n', C14)"],"metadata":{"id":"pYOcVVxsOXrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_podado = X_train.loc[:, ~X_train.columns.isin(C14)]\n","X_test_podado = X_test.loc[:, ~X_train.columns.isin(C14)]\n","print(f'Las columnas que conserva el dataset son: {list(X_podado.columns)} \\n')\n","model = GaussianNB()\n","\n","model.fit(X_podado, y_train)\n","\n","y_pred = model.predict(X_test_podado)\n","\n","\n","f1 = f1_score(y_test, y_pred, average=\"weighted\")\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred, pos_label=1)\n","recall = recall_score(y_test, y_pred, pos_label=1)\n","\n","print('Las metricas que se obtienen de entrenar al modelo solo con los atributos nombrados arriba son: ')\n","print('Accuracy:', accuray)\n","print('F1 Score:', f1)\n","print('Recall :', recall)\n","print('Precision :', precision)\n","\n","\n","labels = [\"Fully Paid\", \"Not fully Paid\"]\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","\n","print('La matriz de confusón es:')\n","\n","disp.plot()"],"metadata":{"id":"0-3SVN56Ohh5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##f"],"metadata":{"id":"dPBid6KQOlkT"}},{"cell_type":"markdown","source":["La conclusión a la que llego con el item (e) es que a veces es conveniente reducir la dimensionalidad de lo datos, eliminando atributos redundantes e incluso entorpecedores a la hora de entrenar mi modelo para clasificación. Lo que mas me sorprendio es que reduciendo la dimensionalidad de los datos obtuve resultados notoriamente mejores (¡y eso que elimine la mayoria de los atributos del dataset original!), antes de esto yo pensaba que reduciendo la dimensionalidad se podia obtener como mucho resultados iguales que antes, esto me parece super util para futuros programas y trabajos.\n"],"metadata":{"id":"EvH0WEO3Ooc3"}}]}